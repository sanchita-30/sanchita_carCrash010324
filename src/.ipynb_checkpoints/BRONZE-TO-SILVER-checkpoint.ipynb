{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8195eaeb-36ce-476a-bc9a-521ee3d443e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./COMMON/FUNCTIONS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b37c76c5-8e2e-4ca8-b2ff-ffbff1338105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToSilver():\n",
    "    #data cleaning functions\n",
    "    def __init__(self, PATH):\n",
    "        ## ADD BAD RECORD PATH\n",
    "        self.df = spark.read.format(\"csv\").option(\"Header\",True).option(\"inferSchema\",True).load(PATH)\n",
    "       \n",
    "    def show(self):\n",
    "        self.df.show()\n",
    "\n",
    "    def nullHandling(self):\n",
    "        '''\n",
    "            Function responsible to handle nulls\n",
    "            int: 0\n",
    "            string: NA\n",
    "            datetime: 1900-01-01\n",
    "        '''\n",
    "        \n",
    "        string_col = [col[0] for col in self.df.dtypes if col[1].startswith('string')]\n",
    "        int_col = [col[0] for col in self.df.dtypes if col[1].startswith('int')]\n",
    "        timestamp_col = [col[0] for col in self.df.dtypes if col[1].startswith('timestamp')]\n",
    "        \n",
    "        return self.df.fillna('NA', subset = string_col)\\\n",
    "                .fillna(0, subset = int_col).fillna('1900-01-01',subset = timestamp_col)\n",
    "        \n",
    "    def dropColumns(self,df,columns):\n",
    "        '''\n",
    "            Parameters\n",
    "                df: Dataframe\n",
    "                columns: Column list to select\n",
    "        '''\n",
    "        df1 = df.select(columns).drop_duplicates()\n",
    "        return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f58eed3f-3f83-41c0-ac3b-1717dd82e20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "    for key in BRONZE_PATHS.keys():\n",
    "        path = BRONZE_PATHS[key]\n",
    "        dfname = key[:-9]\n",
    "        obj = ToSilver(path)\n",
    "        df = obj.nullHandling()\n",
    "        df = obj.dropColumns(df,COLUMNS[dfname+'_COLUMNS'])\n",
    "        df.write.option(\"header\",True).mode('overwrite').csv('C:/Users/sanch/PROJECTS/CAR-CRASH-SDE/DATA/SILVER/'+dfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe470b0-90a3-4fdf-8a36-cf9b5d6b57ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
