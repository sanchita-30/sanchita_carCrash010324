{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8195eaeb-36ce-476a-bc9a-521ee3d443e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./UTILS/modules.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b37c76c5-8e2e-4ca8-b2ff-ffbff1338105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToSilver():\n",
    "    #data cleaning functions\n",
    "    def __init__(self, PATH):\n",
    "        ## ADD BAD RECORD PATH PERMISSIVE\n",
    "        self.df = spark.read.format(\"csv\").option(\"Header\",True).option(\"inferSchema\",True).load(PATH)\n",
    "\n",
    "    def nullHandling(self):\n",
    "        '''\n",
    "            Function responsible to handle nulls\n",
    "            int: 0\n",
    "            string: NA\n",
    "            datetime: 1900-01-01\n",
    "        '''\n",
    "        \n",
    "        string_col = [col[0] for col in self.df.dtypes if col[1].startswith('string')]\n",
    "        int_col = [col[0] for col in self.df.dtypes if col[1].startswith('int')]\n",
    "        timestamp_col = [col[0] for col in self.df.dtypes if col[1].startswith('timestamp')]\n",
    "        \n",
    "        return self.df.fillna('NA', subset = string_col)\\\n",
    "                .fillna(0, subset = int_col).fillna('1900-01-01',subset = timestamp_col)\n",
    "        \n",
    "    def dropColumns(self,df,columns):\n",
    "        '''\n",
    "            Parameters\n",
    "                df: Dataframe\n",
    "                columns: Column list to select\n",
    "        '''\n",
    "        df1 = df.select(columns).drop_duplicates()\n",
    "        return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f58eed3f-3f83-41c0-ac3b-1717dd82e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARGES written to Bronze.\n",
      "DAMAGES written to Bronze.\n",
      "ENDORSE written to Bronze.\n",
      "PRIMARY_PERSON written to Bronze.\n",
      "RESTRICT written to Bronze.\n",
      "UNITS written to Bronze.\n"
     ]
    }
   ],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "    config = read_yaml()\n",
    "    for key in config['BRONZE_PATHS']:\n",
    "        \n",
    "        path = config['BRONZE_PATHS'][key]\n",
    "        dfname = key[:-9]\n",
    "        columns = config['COLUMNS'][dfname+'_COLUMNS']\n",
    "        silver_path = config['SILVER_PATHS'][key]\n",
    "        \n",
    "        obj = ToSilver(path)\n",
    "        df = obj.nullHandling()\n",
    "        df = obj.dropColumns(df,columns)\n",
    "        write_df_to_parquet(df,silver_path)\n",
    "        print(dfname,'written to Bronze.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff942b9-5f4a-43d7-9703-070791a489a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
